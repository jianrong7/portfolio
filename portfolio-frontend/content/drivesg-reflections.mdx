---
title: "DriveSG - Reflections"
subtitle: Follow along as I reflect on my journey building a native application using React Native.
date: "2021-08-30T08:00:00.000Z"
updated: "2021-08-30T08:00:00.000Z"
categories: []
keywords: ["react native", "self-improvement"]
slug: drivesg-reflections
type: "blogPost"
colorFeatured: "linear-gradient(104.01deg, #9BEBEB 5.51%, #0FA6E9 98.93%)"
featured: false
readTime: "2 min read"
---

# **📝** DriveSG Reflections

## 🔑 Why I started building this app

I wanted to work on a project using my newly learnt [React Native](https://reactnative.dev/) skills from [Full Stack Open](https://fullstackopen.com/en/). At that time, I was also practicing for driving theory tests in Singapore. The process was pretty boring. It goes something like this.

1. Read the Highway code book
2. Search up the Internet for [practice resources](https://www.tptest.sg/)
3. Sign up for practice tests
4. Go for the test

Since I managed to get additional resources, I thought I could make them into extra practice for people like me, who want to do the test at the convenience of their home or anywhere they are. That would give people more confidence when they were actually going to do the actual test.

I thought that this app would be a great combination of both worlds. I get to practice my RN skills while helping out others who wanted to get their driving license.

## 🏗️ How I built it

### Front End

I built this using [React Native](https://reactnative.dev/), through the [Expo CLI](https://docs.expo.dev/index.html). I have been learning frontend development for almost half a year now. When I heard that I could use my [React.js](https://reactjs.org/) skills to create a mobile app, I got very excited. However, after going through some tutorials, I realize the limitations of that but I decided that it was still good enough for my not-so-complicated app. I wanted to use it to showcase my skills after going through [The Odin Project](https://www.theodinproject.com/) and [Full Stack Open](https://fullstackopen.com/en/).

The routing and navigation was largely handled by [React Navigation](https://reactnavigation.org/). Although it offered pretty good UI and functionality out of the box, I was a bit limited by its customizability. I still stuck with it because I felt like the trade-off was fair and it got the app up and running very quickly. In the end, I am glad I used the library, my experience with it was generally positive although I still cannot remove the border from the header after scrolling through countless of [Github threads](https://github.com/react-navigation/react-navigation/issues/865).

### Back End

When I was deciding what to use for my backend, I originally wanted to build my own REST API using [Express](https://expressjs.com/) and [MongoDB](https://www.mongodb.com/). I thought that it would be helpful because others could potentially fetch data from the API in the future when they want to create their own application. I even thought about using [GraphQL](https://graphql.org/). In the end, however, I used neither. GraphQL was overkill because this app did not require me to worry about over or underfetching of data. I only needed to fetch 50 questions and answers every time the user attempts a test.

Keeping in mind that I wanted the application quickly up and running, I decided to use [Firebase](https://firebase.google.com/). Its simplicity made the development process a breeze and I am glad I picked a BaaS to complete this project. In the future, maybe I will look into creating my own REST API which can be more flexible.

## 😔 Problems I faced

### Extracting text from image

One of the biggest problems I faced was extracting the text from images. Originally all my questions are in image format. This meant that I had to find a way to extract text from images. Naturally, Google told me to use Optical Character Recognition (OCR) technology. My first thought was to go find a JS library that do just that. In the end, I used [Tesseract.js](https://tesseract.projectnaptha.com/). I thought the magical AI tools can help me extract my text perfectly. Boy, how wrong was I.

The text extracted had lots of empty spaces and gibberish because the text were sparse and their fonts were also inconsistent. I went to search up on how to improve the accuracy of my results and several suggestions pointed me to use OpenCV. I thought about diving down the rabbit hole of Artifical Intelligence, but I realised that was not my goal. My goal was to create a working prototype quickly and it did not need to be 100% accurate.

In the end, I used a very crude method of dividing all the images into several rectangles and used Tesseract.js to extract the text for me. The results, though not perfect, were much better.

### Cropping images from an image

Another problem I faced was trying to get the images from the questions. As some questions had images, I had to do a mass cropping of the images. I thought about using CropperJS but in the end I found a shorter workaround of using Photoshop's Automate Batch jobs. It did the trick after watching a tutorial video.

### Linking pictures from Storage to Firestore

As Firebase Storage did not support NodeJS, I faced a problem in linking the images from Firebase Storage to my questions in Cloud Firestore. In the end, I found a workaround whereby I listed all the files in Firebase Storage and then used NodeJS to update the documents in Firestore with the image links.

## 📕 What I learnt

Applying the technology in real life isn't as straightforward as I thought. There are many problems that will come up unexpectedly along the way. As much as possible, plan out the architecture of the project, but be mentally ready and flexible enough to learn new technologies (Photoshop, Figma) to solve the problems at hand.

This project also taught me that one developer alone can actually complete a lot but it is also not the most ideal because I kept asking my friends for opinions, whether it was related to its design or its functionality.

Also, the last 10% of the project took up 90% of my time. Things like fixing bugs, removing input error, making the buttons recognize a touch slightly out of the button. These things take up the most time. Although they do not add significantly to the core functionality of the app, they contribute massively to the user experience.

This was also my first time pushing an application to the Google Play Store. Thanks to Expo CLI, I got the App Bundle relatively quickly, however, the process of reviewing the app and getting the actual working product uploaded to the app store was relatively slow. It taught me the importance of planning as well as the emphasis on testing the application before the actually publishing the application.

## 🤔 What's next?

DriveSG is definitely not a complete product. There are still certain places that I will touch up here and there. But it is mostly usable and can be considered a Minimum Viable Product (MVP).

Like what Sheryl Sandberg said, "Done is better than perfect."

I am glad to be able to ship the app to production. The job is done, but it is far from perfect.

- Implement my own backend using Express.js and MongoDB
- User authentication system to save history of user attempts

## 🍎 Getting the iOS version running

One of the main reasons I used React Native was because I wanted a mobile application which worked on both Android and iOS devices while sharing a common codebase. Thankfully, the process was very smooth, largely because this application is not very heavy but also because the Expo CLI made the process much smoother with their detailed guides. Within an hour, I could get the production application up and running on the iOS simulator. It was really fascinating to see my application working on two completely different platforms.

The React Native team is doing a great job and I would definitely use React Native more in the future. Being able to transfer my React and JavaScript knowledge directly into building a mobile application is really convenient, and I think most developers believe that developer experience triumphs all.

Although the React Native hype is no longer as great, the RN team has still been very active, keeping RN updated and consistently posting their vision for RN. It is also nice to see that RN is being used by parts of the Facebook application, showing that it is still feasible to use RN for a large scale application across platforms. In tandem with [Facebook's vision to become 'a metaverse company'](https://www.theverge.com/22588022/mark-zuckerberg-facebook-ceo-metaverse-interview), the RN developers are also trying to make RN for VR. With the competition from Flutter, the RN team continues to show their determination to continuously innovate on their framework and improve it.

### Technologies used

- React Native
  - React Navigation
  - React Native Progress
  - Expo CLI
- Firebase
  - Cloud Firestore
  - Storage
- Figma
  - To design logo and splash screen
- Photoshop
  - To crop multiple images
- Tesseract.js
  - Used OCR to extract text from images
